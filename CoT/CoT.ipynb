{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Chain-of-Thoughts (CoT) prompting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoT prompting in NLP used to guide the responses generated by language models, often improves interpretability and accuracy for more complex queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Differential between prompting and CoT prompting:**\n",
    "\n",
    "- Standard Prompting: Direct, concise answer generation from question (`input-output` pairs) without explanation or intermediate steps .\n",
    "\n",
    "- CoT Prompting: The model walks through its thought process `input-CoT-output` step-by-step to reach an answer.\n",
    "\n",
    "![image.png](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*gwANOWpyuNX2xuqd0wgoHg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Innovation of CoT Prompting**\n",
    "\n",
    "- **Transformer-based language models** like GPT-3 \n",
    "    - (through `autoregressive modeling`) excel at:\n",
    "        - predicting the next token in a sequence.\n",
    "        - generating fluent text. \n",
    "    - are bad at: tasks requiring reasoning or multi-step problem-solving.\n",
    "\n",
    "`Note:` `autoregressive modeling` is to generates text by selecting the most probable next word or token based on the context of the previous words.\n",
    "\n",
    "- **Standard prompting** with further limits the model's reasoning ability.\n",
    "\n",
    "- **Scaling model size** has improved performance:\n",
    "    - But does **NOT** inherently address the reasoning challenges on standard prompting.\n",
    "    - For CoT prompting to work effectively, the model must have **reached a certain size** (100B parameters) where it unlock complex cognitive functions.\n",
    "\n",
    "- **CoT prompting** outperforms across benchmarks:\n",
    "    - arithmetic reasoning (GSM8K dataset)\n",
    "    - commonsense reasoning (CSQA, StrategyQA)\n",
    "    - symbolic reasoning (coin flip task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic knowledge on PaLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Pathways Language Model (PaLM)** with 540B parameters:\n",
    "- is based on the Transformer architecture, utilizing a decoder-only setup where each time step can attend to itself and previous steps.\n",
    "- introduced CoT prompting.\n",
    "\n",
    "**Memorization**\n",
    "\n",
    "Neural networks are prone to memorizing portions of their training data, a behavior linked to overfitting. \n",
    "\n",
    "Larger models tend to exhibit higher rates of memorization, particularly for rare examples encountered only once during training; reducing generalization to new data.\n",
    "\n",
    "This presents a challenge in training large-scale models, despite efforts to mitigate it through techniques like regularization and data augmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the properties of CoT Prompting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Decomposition of Multi-Step Problems**: \n",
    "    - break down complex, multi-step problems into manageable intermediate steps. \n",
    "    - allocate additional computational resources to problems that require more intricate reasoning.\n",
    "\n",
    "2. **Interpretability of Model Behavior**: \n",
    "    - provides a clearer, more interpretable view into how a model arrives at a particular answer. \n",
    "    - identify the reasoning process and highlights potential points of error.\n",
    "    - fully understanding the model’s computations supporting an answer remains a challenge.\n",
    "\n",
    "3. **Versatility Across Tasks**: including math word problems, commonsense reasoning, and symbolic manipulation.\n",
    "\n",
    "4. **Ease of Elicitation in Large Models**: by incorporating examples of such reasoning sequences into prompts (called few-shot learning prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*M3Oe3aEiVp4ikPXU0ugRAg.png' width='600'>\n",
    "\n",
    "**Tree-of-Thought (ToT)**:\n",
    "- is an improvement of CoT.\n",
    "- is a tree search approach.\n",
    "- explore multiple potential reasoning paths.\n",
    "- use state evaluation and backtracking to ensure more accurate outcomes.\n",
    "\n",
    "**Why need ToT?**\n",
    "- LLMs (decoder models) generate token by token.\n",
    "- They may follow an incorrect path when choosing subsequent words for the answers.\n",
    "- They often lack the ability to recover from such missteps.\n",
    "\n",
    "**ToT mechanism**\n",
    "1. generate multiple potential “first thoughts”.\n",
    "2. selects the most promising option.\n",
    "3. generates alternative options for the next steps.\n",
    "4. weak or incorrect answers are discarded.\n",
    "\n",
    "**Cons of ToT**: takes time as the model being invoked multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arithmetic Reasoning in CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*931C0NW9MbjzGNAAbxkong.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commonsense Reasoning in CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g., \n",
    "`Query:` “Sammy wants to go where there are many people. Where might he go? \n",
    "(a) desert, (b) racetrack, (c) populated areas, (d) apartment,” \n",
    "\n",
    "CoT prompting would lead the model to reason step-by-step as follows:\n",
    "\n",
    "1. **Initial understanding**: “Sammy wants to go to a place with many people.”\n",
    "2. **Evaluation of options**: “The desert and racetrack are not places with many people. Apartments have some people but not many in comparison.”\n",
    "3. **Conclusion**: “Populated areas have many people, so the answer is © populated areas.”\n",
    "\n",
    "This process breaks down the reasoning into intermediate steps that make it easier for the model to handle tasks that require more than just recalling facts.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*8M6MG_9rQZE2HyqhT_emMA.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Reasoning in CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symbolic reasoning involves tasks such as performing operations on variables, manipulating sequences of symbols, or following abstract rules.\n",
    "\n",
    "e.g., `Query`: Concatenate the last letters of the words ‘Apple’ and ‘Banana’.\n",
    "\n",
    "With CoT prompting, the model is guided to think through each step explicitly:\n",
    "\n",
    "1. Step 1: “The last letter of ‘Apple’ is ‘e’.”\n",
    "2. Step 2: “The last letter of ‘Banana’ is ‘a’.”\n",
    "3. Step 3: “Concatenate ‘e’ and ‘a’ to form ‘ea’.”\n",
    "4. Conclusion: “The answer is ‘ea’.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating step-by-step reasoning increases the number of tokens the model must process  => requires more computational resources and time.\n",
    "- CoT is inaccessible to smaller, less resource-intensive models (< 100B params).\n",
    "- CoT is not infallible.\n",
    "- CoT's effectiveness in abstract reasoning tasks remains an open question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concise Chain-of-Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concise Chain of Thought (CCoT) is a combination of **Chain of Thought (CoT) prompting** and **Concise prompting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concise Prompting**\n",
    "- instructs models to produce answers that are as brief as possible without sacrificing the essential reasoning or content required for a correct solution.\n",
    "- `Pros:` reduces the number of tokens produced, which directly correlates with lower computational and financial costs.\n",
    "- `Cons:` negatively impact the performance, especially in tasks that benefit from more elaborate reasoning steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concise Chain of Thought (CCoT)**\n",
    "- combines the **step-by-step** problem-solving approach of CoT with the efficiency-oriented approach of concise prompting.\n",
    "- Few-shot examples, provided to the model during prompting in CCoT, are written **concisely**.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*8TvudEI1KmwVT9x-1z7m7Q.jpeg' alt='ToT' width='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits of CCoT**\n",
    "- Reduced Token Costs\n",
    "- Faster Responses\n",
    "- Efficiency without Significant Performance Drop\n",
    "\n",
    "**Limitations of CCoT**\n",
    "- Performance Penalty in Specific Domains: CCoT is not suitable for problems that require detailed reasoning steps.\n",
    "- Dependence on Model Sophistication.\n",
    "- Generalization Across Models: CCoT has mainly been tested on the GPT model, not tested on other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic inference code on CoT reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dangc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'EleutherAI/gpt-neo-125M' # or \"EleutherAI/gpt-neox-20b\", GPT-3, GPT-4\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input prompt with CoT reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "Q: There are 3 apples, and you take away 2. How many apples do you have now? \n",
    "Let's think step by step. \n",
    "First, there are 3 apples in total.\n",
    "Then, you take away 2 apples.\n",
    "So, you have taken 2 apples with you.\n",
    "Therefore, the answer is 2.\n",
    "A: 2\n",
    "\n",
    "Q: If a train leaves the station at 3 PM and travels 60 miles per hour, how far will it have traveled by 6 PM? \n",
    "Let's break it down step by step.\n",
    "First, the train travels for 3 hours from 3 PM to 6 PM.\n",
    "In each hour, it travels 60 miles.\n",
    "So, in 3 hours, it will have traveled 60 * 3 = 180 miles.\n",
    "Therefore, the train will have traveled 180 miles by 6 PM.\n",
    "A: 180\n",
    "\n",
    "Q: If a store sells a dozen eggs for $3, how much will 8 dozen eggs cost?\n",
    "Let's solve this step by step.\n",
    "First, 1 dozen eggs costs $3.\n",
    "Therefore, 8 dozen eggs will cost 8 * 3 = $24.\n",
    "So, the total cost for 8 dozen eggs is $24.\n",
    "A: 24\n",
    "\n",
    "Q: John has twice as many brothers as sisters. His sister Ann has the same number of brothers as sisters. How many brothers and sisters are in the family? \n",
    "Let's reason step by step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the input prompt and generate the model's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(**inputs, max_length=300, do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode the generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q: There are 3 apples, and you take away 2. How many apples do you have now? \\nLet's think step by step. \\nFirst, there are 3 apples in total.\\nThen, you take away 2 apples.\\nSo, you have taken 2 apples with you.\\nTherefore, the answer is 2.\\nA: 2\\n\\nQ: If a train leaves the station at 3 PM and travels 60 miles per hour, how far will it have traveled by 6 PM? \\nLet's break it down step by step.\\nFirst, the train travels for 3 hours from 3 PM to 6 PM.\\nIn each hour, it travels 60 miles.\\nSo, in 3 hours, it will have traveled 60 * 3 = 180 miles.\\nTherefore, the train will have traveled 180 miles by 6 PM.\\nA: 180\\n\\nQ: If a store sells a dozen eggs for $3, how much will 8 dozen eggs cost?\\nLet's solve this step by step.\\nFirst, 1 dozen eggs costs $3.\\nTherefore, 8 dozen eggs will cost 8 * 3 = $24.\\nSo, the total cost for 8 dozen eggs is $24.\\nA: 24\\n\\nQ: John has twice as many brothers as sisters. His sister Ann has the same number of brothers as sisters. How many brothers and sisters are in the family? \\nLet's reason step by step.\\nFirst, 1 dozen\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Chain-Of-Thought ( CoT ) in Large Language Models prompting and Concise CoT with Code implementation using Python and PyTorch](https://medium.com/@devmallyakarar/chain-of-thought-cot-in-large-language-models-prompting-and-concise-cot-with-code-82821f9a832d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

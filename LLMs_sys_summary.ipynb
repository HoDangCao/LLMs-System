{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **LLMs** based on the **Transformer Architecture**\n",
    "2. **RAG** (Retrieval Augmented Generation) architecture that pairs generalised LLMs with domain-specific data sets in **Vector Databases**\n",
    "3. **Multi-Agent Systems** combine multiple LLM agents, Vector Database sources and Function Execution to perform complex tasks\n",
    "4. **Open Models** based on either open, free to use weights (i.e Llama from Meta) or open source (i.e. Mixtral from Mistral). These can be fine tuned and allow proprietary low-cost model serving solutions to be developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs (Large Language Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs are advanced AI systems designed to understand and generate text that sounds like it was written by a human. These models use large amounts of data and complex neural networks, like transformers, to perform many language-related tasks.\n",
    "\n",
    "<img src='https://sciforce.solutions/strapi/uploads/02_guide_773e2d4361.jpg' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoregressive LLMs**\n",
    "- generate text by predicting the next word based on the previous words in a sentence.\n",
    "- for tasks:\n",
    "    - Chatbots and Virtual Assistants\n",
    "    - Content Creation Tools\n",
    "    - Predictive Text and Autocomplete\n",
    "    - Language Translation Services\n",
    "    - Interactive Storytelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoencoding LLMs**\n",
    "- focus on learning the structure and meaning of a text.\n",
    "- compress the input text into a simpler, lower-dimensional representation (encoding)\n",
    "- then reconstruct it back to the original form (decoding)\n",
    "- for tasks:\n",
    "    - Text Classification\n",
    "    - Sentiment Analysis\n",
    "    - Information Retrieval\n",
    "    - Anomaly Detection\n",
    "    - Text Summarization\n",
    "    - Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid LLM**\n",
    "- leverage the strengths of both autoregressive and autoencoding models.\n",
    "- generate text while also deeply understanding the context.\n",
    "- for tasks:\n",
    "    - Conversational Agents\n",
    "    - Advanced Content Creation\n",
    "    - Comprehensive Text Analysis\n",
    "    - Interactive Storytelling\n",
    "    - Intelligent Search Engines\n",
    "    - Personalized Recommendations\n",
    "    - Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Private LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets used typically range from hundreds of terabytes to multiple petabytes.\n",
    "\n",
    "<img src='https://sciforce.solutions/strapi/uploads/03_guide_b1a09510db.jpg' width='400'>\n",
    "\n",
    "- **Web Data**: FineWeb (not fully deduplicated for better performance, entirely English), Common Crawl (55% non-English)\n",
    "\n",
    "- **Code**: Publicly Available Code from all the major code hosting platforms\n",
    "\n",
    "- **Academic Texts**: Anna’s Archive, Google Scholar, Google Patents\n",
    "\n",
    "- **Books**: Google Books, Anna’s Archive\n",
    "\n",
    "- **Court Documents**: RECAP archive (USA), Open Legal Data (Germany)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "\n",
    "is the process of breaking down text into tokens: words, subwords, or characters.\n",
    "\n",
    "<img src='https://sciforce.solutions/strapi/uploads/04_llm_aea9a8fcc4.jpg' width='400'>\n",
    "\n",
    "Help model:\n",
    "- handle various text lengths.\n",
    "- easily manage a set of words or subwords.\n",
    "- understand the context of each token within a sentence.\n",
    "- improve the accuracy of tasks like translation or text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding**\n",
    "\n",
    "turn each text into a unique set of numbers called a vector that captures its meaning so a computer can understand it. e.g.\n",
    "\n",
    "<img src='https://sciforce.solutions/strapi/uploads/05_guide_0369548893.jpg' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention**\n",
    "Helps the model understand which words matter most to get the meaning right.\n",
    "\n",
    "<img src='https://sciforce.solutions/strapi/uploads/06_guide_cfc59c7660.jpg' width='400'>\n",
    "\n",
    "- The green attention reflects positive feedback.\n",
    "- The red attention reflects negative feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Input and Preparation**\n",
    "\n",
    "1. **Data Ingestion**: Collect and load data from various sources.\n",
    "2. **Data Cleaning:** Remove noise, handle missing data, and redact sensitive information.\n",
    "3. **Normalization**: Standardize text, handle categorical data, and ensure data consistency.\n",
    "4. **Chunking**: Split large texts into manageable chunks while preserving context.\n",
    "5. **Tokenization**: Convert text chunks into tokens for model processing.\n",
    "6. **Data Loading**: Efficiently load and shuffle data into batches for optimized training, using parallel loading when necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**\n",
    "\n",
    "Tuning **key hyperparameters** is essential to ensure the training loop converges effectively, leading to better model performance and efficiency.\n",
    "- **Learning Rate**: Determines the size of weight updates during training.\n",
    "\n",
    "<img src='https://sciforce.solutions/strapi/uploads/07_guide_5e1f6d07c5.jpg' width='500'>\n",
    "\n",
    "- **Batch Size**: the number of samples processed in each iteration. \n",
    "    - Larger batches stabilize training but require more memory.\n",
    "    - smaller batches introduce variability but are less resource-intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallelization and Resource Management**\n",
    "\n",
    "LLMs grow in size, so parallelization and resource management techniques become essential for speeding up processing and enabling efficient handling of large data.\n",
    "- **Data Parallelization** splits datasets across multiple GPUs during forward propagation.\n",
    "- **Model Parallelization** divides itself across GPUs, ensuring that all model components are utilized without memory issues.\n",
    "- **Gradient Checkpointing** reduces memory usage during forward propagation, enabling more efficient backward propagation by selectively storing intermediate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iteration and Epochs** (through each data batch):\n",
    "1. Do prediction (forward propagation).\n",
    "2. Calculate training loss.\n",
    "3. Calcualte loss gradient (backward propagation).\n",
    "4. update the model weights.\n",
    "5. Calculate validation loss.\n",
    "\n",
    "Through repeated iterations and multiple epochs:\n",
    "- the model’s parameters are fine-tuned, leading to increasingly accurate and robust.\n",
    "- prevent issues such as overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technical Evaluation** throught benchmarks:\n",
    "- **MMLU (Massive Multitask Language Understanding)**:\n",
    "    - measures the model’s natural language understanding across a broad range of subjects\n",
    "    - is used to assess the general linguistic and reasoning capabilities of LLMs.\n",
    "\n",
    "- **GPQA (General Purpose Question Answering)**:\n",
    "    - evaluate the ability to handle diverse and complex question-answering tasks.\n",
    "    - tests the proficiency in providing accurate, contextually relevant answers across various domains.\n",
    "\n",
    "- **MATH**:\n",
    "    - test the mathematical reasoning skills.\n",
    "    - involves solving multi-step problems that require both calculation and logical reasoning.\n",
    "\n",
    "- **HumanEval**:\n",
    "    - Assesses the capability in generating functional and correct code.\n",
    "\n",
    "- **Arena** (advanced benchmarks and platforms):\n",
    "    - pose questions to two anonymous LLMs and determine which one answers better (LLMs ranking).\n",
    "    - provides a more dynamic, user-driven evaluation.\n",
    "\n",
    "`Note`: \n",
    "- Fine-tuning typically involves adapting the model to specific prompts and contexts.\n",
    "- metric should reflect the business objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversational Performance Evaluation**\n",
    "\n",
    "The engagement, coherence, and context awareness metric measure how effectively the model engages with users and maintains conversation quality.\n",
    "\n",
    "<img src='https://sciforce.solutions/strapi/uploads/08_guide_f1eb9f3dc9.jpg' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continuous Monitoring**\n",
    "- ensure that the model maintains its performance over time, especially as new data becomes available or as the model is deployed in different contexts.\n",
    "- As new data is introduced, periodically retrain and fine-tune the model to keep it accurate and relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG adds extra contextual data into promt\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*qbkE0RhqQw8AH6N9EeMnnw.png' width=600>\n",
    "\n",
    "**Explaination**:\n",
    "- The red arrow path:\n",
    "    - split documents in data source into small chunks.\n",
    "    - pass chunks through an embedding model to get embedded chunks.\n",
    "    - store the embedded chunks into vector DB.\n",
    "- The remaining path:\n",
    "    - pass user's query through the embedding model to get embedded query.\n",
    "    - calculate the similarity score between the embedded query and each embedded chunk.\n",
    "    - combine the chunk with the highest score and the query to get prompt.\n",
    "    - pass the prompt into a LLM model to get the final response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-agent systems are where multiple smaller models collaborate to achieve goals that are unattainable by single models alone.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cJT7HJAhLZHx8zVw2jdsxQ.png' width=500>\n",
    "\n",
    "This structure enables aspects:\n",
    "\n",
    "- Guardrails to check output quality — techniques such as LLM-as-a-judge can check for signs of hallucination or inappropriate responses.\n",
    "- Enable automation process flows by calling functions.\n",
    "- Include a human-in-the-loop to provide feedback on the responses being provided and to make an expert assessment of automated content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantization** is a method of compressing a larger size model (LLM or any deep learning model) to a smaller size.\n",
    "\n",
    "**Quantization** maps the model’s weight value, parameters, and activations from higher precision (eg. FP32) to lower precision (eg. FP16|BF16|INT8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Asymmetric linear quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method maps the values from the original tensor range (Wmin, Wmax) to the values in the quantized tensor range (Qmin, Qmax).\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*2PR4w80jx4Eem3ouVleZFg.png' width='500'>\n",
    "\n",
    "- **Original tensor (W)**\n",
    "- **Quantized tensor (Q)**\n",
    "- **Scale value (S)**:\n",
    "    - scales down the values of the original tensor during quantization.\n",
    "    - scales up the value of the quantized tensor uring dequantization.\n",
    "    - The data type of scale value is the same as the original tensor.\n",
    "- **Zero point (Z)**: \n",
    "    - is the non-zero value in the quantized tensor range that directly gets mapped to the value 0 in the original tensor range.\n",
    "    - The data type of zero-point is the same as the quantized tensor.\n",
    "\n",
    "**Quantization Process**\n",
    "\n",
    "**Step 1: Calculate de-quantized (origin) tensor value:**\n",
    "$$W=S(Q-Z) \\text{        (eq 1)}$$\n",
    "\n",
    "**Step 2: Calculate scale value:**\n",
    "$$\\begin{align*}\n",
    "W_{max}-W_{min} &= S(Q_{max}-Z)-S(Q_{min}-Z) \\\\\n",
    "&= S(Q_{max}-Q_{min}) \\\\\n",
    "\\Rightarrow S &= \\frac{W_{max}-W_{min}}{Q_{max}-Q_{min}}  \\text{        (eq 2)}\n",
    "\\end{align*}$$\n",
    "\n",
    "**Step 3: Calculate zero point value:**\n",
    "$$W_{min} = S(Q_{min}-Z) \\Rightarrow Z = Q_{min}-\\frac{W_{min}}{S}$$\n",
    "Z should have the data type of INT8, so rounded and converted to INT type.\n",
    "$$Z = int\\left(round\\left(Q_{min}-\\frac{W_{min}}{S}\\right)\\right) \\text{        (eq 3)}$$\n",
    "\n",
    "**Step 4: Calculate quantized tensor value:**\n",
    "$$W = S(Q-Z) \\Rightarrow Q = \\frac{S}{Q} + Z$$\n",
    "Q should have the data type of INT8.\n",
    "$$Q = int\\left(round\\left(\\frac{S}{Q} + Z\\right)\\right) \\text{        (eq 4)}$$\n",
    "\n",
    "**Issue: Zero point (Z) or Quantized tensor value (Q) runs out of the range** \n",
    "\n",
    "`Solution`: Change the value of Z or Q to $Q_{min}$ if it is smaller than $Q_{min}$ and to $Q_{max}$ if it is bigger than $Q_{max}$.\n",
    "\n",
    "`Note:` The range of value is\n",
    "- (-128, 127) for INT8 - signed integer.\n",
    "- (0, 255) for UINT8 - unsigned integer.\n",
    "\n",
    "**Cons:** have parameter Z, so higher memory footprint than symmetric quantization in case of large model.\n",
    "\n",
    "**recommend for**: quantization to 4bit, 2bit or 1bit integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Symmetric linear quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the symmetric method, the 0 point in the original tensor range maps to the 0 point in the quantized tensor range.\n",
    " \n",
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*zsy-J4_-8qyLV_yXMgYBBA.png' width=500>\n",
    "\n",
    "**Quantization Process**\n",
    "\n",
    "**Step 1: Calculate de-quantized (origin) tensor value:**\n",
    "$$W=S(Q) \\text{        (eq 1)}$$\n",
    "\n",
    "**Step 2: Calculate scale value:**\n",
    "$$W_{max} = SQ_{max} \\Rightarrow S = \\frac{W_{max}}{Q_{max}}  \\text{        (eq 2)}$$\n",
    "\n",
    "**Step 3: Calculate quantized tensor value:**\n",
    "$$W = S(Q) \\Rightarrow Q = \\frac{W}{S}$$\n",
    "Q should have the data type of INT8.\n",
    "$$Q = int\\left(round\\left(\\frac{W}{S}\\right)\\right) \\text{        (eq 3)}$$\n",
    "\n",
    "**Cons:** if the dataset is not distributed properly that will result in large unused section in quantized range.\n",
    "\n",
    "**recommend for**: quantization to 8bit integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize and de-quantize LLM weight parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a quick look at how weight parameter values change after quantization in the transformer model.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*5MnWAtVmMCzOWAgzwd-8xg.png' width=500>\n",
    "\n",
    "Example of FP32, INT8, UINT8 data type distribution and calculation\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*X3xu6Msz36b0IcL3iRCNJw.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymmetric quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the original weight tensor (datatype: FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1538,  1.4439, -1.7790, -0.1252],\n",
      "        [ 0.5709, -0.0624, -2.9413,  0.8313],\n",
      "        [-1.1088, -0.3239, -0.0625,  0.9305],\n",
      "        [ 1.0707,  1.0060,  0.3864, -0.8464]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "original_weight = torch.randn((4,4))\n",
    "print(original_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define quantization and de-quantization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmetric_quantization(original_weight):\n",
    "    # define the data type to quantize. In our example, it's INT8.\n",
    "    quantized_data_type = torch.int8\n",
    "\n",
    "    # Get the Wmax and Wmin value from the orginal weight which is in FP32.\n",
    "    Wmax = original_weight.max().item()\n",
    "    Wmin = original_weight.min().item()\n",
    "\n",
    "    # Get the Qmax and Qmin value from the quantized data type. \n",
    "    Qmax = torch.iinfo(quantized_data_type).max\n",
    "    Qmin = torch.iinfo(quantized_data_type).min\n",
    "\n",
    "    # Calculate the scale value using the scale formula. Datatype - FP32.\n",
    "    S = (Wmax - Wmin)/(Qmax - Qmin)\n",
    "\n",
    "    # Calculate the zero point value using the zero point formula. Datatype - INT8.\n",
    "    Z = Qmin - (Wmin/S)\n",
    "    \n",
    "    # Check if the Z value is out of range.\n",
    "    Z = max(Qmin, Z)\n",
    "    Z = min(Qmax, Z)\n",
    "    Z = int(round(Z))\n",
    "\n",
    "    # Calculate the quantized weight.\n",
    "    quantized_weight = (original_weight/S) + Z\n",
    "\n",
    "    # Ensure the quantized weight doesn't goes out of range [Qmin, Qmax].\n",
    "    quantized_weight = torch.clamp(torch.round(quantized_weight), Qmin, Qmax)\n",
    "\n",
    "    # cast the datatype to INT8.\n",
    "    quantized_weight = quantized_weight.to(quantized_data_type)\n",
    "\n",
    "    return quantized_weight, S, Z\n",
    "\n",
    "def asymmetric_dequantization(quantized_weight, scale, zero_point):\n",
    "    # Use the dequantization calculation formula derived in the math section of this post.\n",
    "    # Also make sure to convert quantized_weight to float as substraction between two INT8 values (quantized_weight and zero_point) will give unwanted result. \n",
    "    dequantized_weight = scale * (quantized_weight.to(torch.float32) - zero_point)\n",
    "\n",
    "    return dequantized_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized weight: tensor([[ -24,  127,  -60,   36],\n",
      "        [  76,   39, -128,   91],\n",
      "        [ -21,   24,   39,   97],\n",
      "        [ 105,  102,   65,   -6]], dtype=torch.int8)\n",
      "scale: 0.01719683665855258\n",
      "zero point: 43\n"
     ]
    }
   ],
   "source": [
    "quantized_weight, scale, zero_point = asymmetric_quantization(original_weight)\n",
    "print(f\"quantized weight: {quantized_weight}\")\n",
    "print(f\"scale: {scale}\")\n",
    "print(f\"zero point: {zero_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1522,  1.4445, -1.7713, -0.1204],\n",
      "        [ 0.5675, -0.0688, -2.9407,  0.8254],\n",
      "        [-1.1006, -0.3267, -0.0688,  0.9286],\n",
      "        [ 1.0662,  1.0146,  0.3783, -0.8426]])\n"
     ]
    }
   ],
   "source": [
    "dequantized_weight = asymmetric_dequantization(quantized_weight, scale, zero_point)\n",
    "print(dequantized_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the quantization error between the de-quantized weight and the origin one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9039e-05)\n"
     ]
    }
   ],
   "source": [
    "quantization_error = (dequantized_weight - original_weight).square().mean()\n",
    "print(quantization_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` The is so much less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the same code with the asymmetric method. The only change is to always ensure the value of zero_input to be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LLMs— From Research to Reality](https://medium.com/@ed.bullen/llms-from-research-to-reality-57d5936552c1)\n",
    "\n",
    "[Step-by-Step Guide to Creating Your Own Large Language Model](https://sciforce.solutions/blog/stepbystep-guide-to-creating-your-own-large-language-model-208)\n",
    "\n",
    "[Building Multi AI Agent Systems: A Comprehensive Guide!](https://ai.plainenglish.io/building-multi-ai-agent-systems-a-comprehensive-guide-58bf21f84f6e)\n",
    "\n",
    "[Want to Learn Quantization in The Large Language Model?](https://pub.towardsai.net/want-to-learn-quantization-in-the-large-language-model-57f062d2ec17)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
